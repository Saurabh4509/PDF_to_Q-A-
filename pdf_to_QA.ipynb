{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) First  step is to  extract  data from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams, LTChar, LTTextBox\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTTextLineHorizontal, LTAnno\n",
    "from pdfminer.high_level import extract_pages\n",
    "import json\n",
    "\n",
    "def is_digit(char):\n",
    "    return char.isdigit()\n",
    "\n",
    "def extract_font_info(layout):\n",
    "    max_font_size = 0  # Variable to store the largest font size\n",
    "\n",
    "    for lt_obj in layout:\n",
    "        if isinstance(lt_obj, LTTextBox):\n",
    "            for text_line in lt_obj:\n",
    "                for character in text_line:\n",
    "                    if isinstance(character, LTChar):\n",
    "                        current_char = character.get_text()\n",
    "\n",
    "                        # Skip digit characters\n",
    "                        if is_digit(current_char):\n",
    "                            continue\n",
    "\n",
    "                        current_font_size = character.size\n",
    "\n",
    "                        # Update max_font_size if the current font size is larger\n",
    "                        if current_font_size > max_font_size:\n",
    "                            max_font_size = current_font_size\n",
    "\n",
    "    return max_font_size\n",
    "\n",
    "def font_size(path):\n",
    "    with open(path, 'rb') as fp:\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "    # Check if the document allows text extraction. If not, abort.\n",
    "    #if not document.is_extractable:\n",
    "        #raise PDFTextExtractionNotAllowed\n",
    "\n",
    "    # Create a PDF resource manager object that stores shared resources.\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "    # Create a PDF device object.\n",
    "        device = PDFDevice(rsrcmgr)\n",
    "\n",
    "    # BEGIN LAYOUT ANALYSIS\n",
    "    # Set parameters for analysis.\n",
    "        laparams = LAParams()\n",
    "\n",
    "    # Create a PDF page aggregator object.\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "    # Create a PDF interpreter object.\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        total_pages = len(list(PDFPage.create_pages(document)))\n",
    "        start_page = int(total_pages * 0.5)\n",
    "        end_page = int(total_pages * 0.6)\n",
    "\n",
    "        largest_font_size = 0  # Variable to store the largest font size across all pages\n",
    "\n",
    "        for page_number, page in enumerate(PDFPage.create_pages(document)):\n",
    "            if start_page <= page_number < end_page:\n",
    "                interpreter.process_page(page)\n",
    "                layout = device.get_result()\n",
    "                current_page_max_font_size = extract_font_info(layout)\n",
    "\n",
    "            # Update largest_font_size if the current page's max font size is larger\n",
    "                if current_page_max_font_size > largest_font_size:\n",
    "                    largest_font_size = current_page_max_font_size\n",
    "\n",
    "    return largest_font_size\n",
    "def extract_pdf(path):\n",
    "    temp_pdf = []\n",
    "    temp_dict = {}\n",
    "    page_data = \"\"  # Initialize page_data\n",
    "    largest_font_size = font_size(path)\n",
    "\n",
    "    for page_layout in extract_pages(path):\n",
    "        for element in page_layout:      # extracting the pages\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                for text_line in element: # extracting the lines in pages\n",
    "                    if isinstance(text_line, LTTextLineHorizontal):\n",
    "                        for chr in text_line:\n",
    "                            if isinstance(chr, LTChar):\n",
    "                            # Check for bold, italic, and hyperlink\n",
    "\n",
    "                                if (chr.size >= largest_font_size):\n",
    "                                    if len(page_data) > 0:\n",
    "                                        temp_dict['data'] = page_data\n",
    "                                        temp_pdf.append(temp_dict)\n",
    "                                        temp_dict = {}\n",
    "                                    temp_dict[\"title\"] = text_line.get_text()\n",
    "                                    page_data = \"\"\n",
    "                                    has_content_started = True\n",
    "                                    break\n",
    "                                elif (chr.size < largest_font_size):\n",
    "                                    page_data += text_line.get_text()\n",
    "                                    break\n",
    "\n",
    "    out_file = open(\"Save the path to save the json file.json\", \"w\", encoding='utf-8')  # Add encoding='utf-8'\n",
    "    json.dump(temp_pdf, out_file, indent=6, ensure_ascii=False)  # Add ensure_ascii=False\n",
    "    out_file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    extract_pdf(r\"Specify  your pdf file location\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:-\n",
    "1) You have to  uplaod your  pdf location\n",
    "2) What ever  data which  is Extrcated from  the pdf is json formate,so  please save data in  JSON  Foramte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2) Set up  groq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install langchain-groq groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"Put_Your_Groq_API_Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, groq_api_key=api_key, model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Now once again  specify  your  Input  location  file, just  for safer  side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"Specify  your JSON file path\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4) Now the Question and Answer part  comes  in  play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# INPUT_PATH = JSON_OUTPUT\n",
    "SEPARATOR = '\\\\'\n",
    "OUTPUT_FILE = f\"{INPUT_PATH.rsplit(SEPARATOR, 1)[1].split('.')[0]}_qna.json\"\n",
    "OUTPUT_PATH = f\"D:\\\\Pdf_Extraction\\\\{OUTPUT_FILE}\"\n",
    "\n",
    "CONTEXT_THRESHOLD_SHORT = 2200\n",
    "CONTEXT_THRESHOLD_LONG = 8000\n",
    "PARTITION_SIZE = 6000\n",
    "\n",
    "def generate_system_prompt() -> str:\n",
    "    \"\"\"Generate the system prompt for the LLM.\"\"\"\n",
    "    return \"\"\"\n",
    "    Imagine you're a super-powered question master! Your mission: transform the context into a unique, captivating, relevant and thought-provoking Questions and Answers session. Here's the key to crafting exceptional questions:\n",
    "        \n",
    "        Language: All the questions and answers should only be generated in Marathi and no other language.\n",
    "        Beyond the Obvious: Dive deeper than surface-level facts. Challenge users to analyze the text's meaning and underlying implications.\n",
    "        Fresh Perspectives: Craft unique questions that avoid simply restating information from the passage. No repeats allowed! And Think outside the box!\n",
    "        Laser Focus: Keep your sights on the core ideas. Don't get bogged down in minor details.\n",
    "        Spark Curiosity: Design questions that ignite users' interest and encourage them to truly grasp the text's essence.\n",
    "        Variety is Key: Mix up question types (factual, analytical, open-ended) to keep the Q&A session engaging.\n",
    "        Challenge Accepted: Push users to think critically and develop their interpretations and connections.\n",
    "        Real-World Relevance: Explore how the text connects to broader concepts and real-world applications.\n",
    "        Prediction Power: Encourage users to predict potential outcomes or consequences based on the information presented.\n",
    "        Creative Spark: For creative content, delve into the author's motivations, potential symbolism, and alternative interpretations.\n",
    "        Go Deeper, Go Further: Don't stop at the first answer! Prompt followup questions to encourage deeper understanding and analysis.\n",
    "    \"\"\"\n",
    "\n",
    "def generate_user_prompt(context: str) -> str:\n",
    "    \"\"\"Generate the user prompt for the LLM.\"\"\"\n",
    "    template = \"\"\"\n",
    "    context: {context}\n",
    "\n",
    "    Q1:-\n",
    "    A1:-\n",
    "\n",
    "    Q2:-\n",
    "    A2:-\n",
    "\n",
    "    Q3:-\n",
    "    A3:-\n",
    "\n",
    "    Q4:-\n",
    "    A4:-\n",
    "\n",
    "    Q5:-\n",
    "    A5:-\n",
    "\n",
    "    Q6:-\n",
    "    A6:-\n",
    "\n",
    "    Q7:-\n",
    "    A7:-\n",
    "\n",
    "    Q8:-\n",
    "    A8:-\n",
    "\n",
    "    Q9:-\n",
    "    A9:-\n",
    "\n",
    "    Q10:-\n",
    "    A10:-\n",
    "\n",
    "    Q11:-\n",
    "    A11:-\n",
    "\n",
    "    Q12:-\n",
    "    A12:-\n",
    "\n",
    "    Q13:-\n",
    "    A13:-\n",
    "\n",
    "    Q14:-\n",
    "    A14:-\n",
    "\n",
    "    Q15:-\n",
    "    A15:-\n",
    "    \"\"\"\n",
    "    return template.format(context=context)\n",
    "\n",
    "def generate_full_prompt(context: str) -> str:\n",
    "    \"\"\"Generate the full prompt for the LLM.\"\"\"\n",
    "    system_prompt = generate_system_prompt()\n",
    "    user_prompt = generate_user_prompt(context)\n",
    "    return f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "def llm_invoke(prompt: str) -> str:\n",
    "    \"\"\"Invoke the LLM with the given prompt.\"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt).content\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking LLM: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_context(context: str) -> list:\n",
    "    \"\"\"Process the context and generate questions.\"\"\"\n",
    "    if len(context) > CONTEXT_THRESHOLD_LONG:\n",
    "        print(\"<==========entered if block==========>\")\n",
    "        parts = [context[i : i + PARTITION_SIZE] for i in range(0, len(context), PARTITION_SIZE)]\n",
    "        responses = []\n",
    "        for part in parts:\n",
    "            prompt = generate_full_prompt(part)\n",
    "            response = llm_invoke(prompt)\n",
    "            if response:\n",
    "                responses.append(response)\n",
    "        return responses\n",
    "    elif len(context) < CONTEXT_THRESHOLD_SHORT:\n",
    "        print(\"<==========entered elif block==========>\")\n",
    "        prompt = generate_full_prompt(context)\n",
    "        response = llm_invoke(prompt)\n",
    "        if response:\n",
    "            return [response]\n",
    "    else:\n",
    "        print(\"<==========entered else block==========>\")\n",
    "        prompt = generate_full_prompt(context)\n",
    "        response = llm_invoke(prompt)\n",
    "        if response:\n",
    "            return [response]\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to process the input data and generate questions.\"\"\"\n",
    "    generated_questions = []\n",
    "    with open(INPUT_PATH, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        total_items = len(data[1:50])\n",
    "        for item in tqdm(data[1:50], desc=\"Processing items\", unit=\"items\"):\n",
    "            title = item['title']\n",
    "            context = item['data']\n",
    "            print(len(context))\n",
    "            questions = process_context(context)\n",
    "            generated_questions.append({\"title\": title, \"context\": context, \"questions\": questions})\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_PATH, \"w\") as f:\n",
    "        json.dump(generated_questions, f, indent=6)\n",
    "\n",
    "    print(f\"Data Extraction Completed and saved at {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5) Use this below,code when you  get  data in  Encoded form,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file content\n",
    "FILE_PATH = OUTPUT_PATH\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    # Directly load the JSON content, no need for decoding\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Function to pretty print the JSON data\n",
    "def pretty_print_json(data):\n",
    "    print(json.dumps(data, ensure_ascii=False, indent=4))\n",
    "\n",
    "# Print the data to verify\n",
    "pretty_print_json(data)\n",
    "\n",
    "# Optional: Save the data to a new JSON file\n",
    "SEPARATOR = \"\\\\\"\n",
    "OUTPUT_FILE = f\"{FILE_PATH.rsplit(SEPARATOR, 1)[1].split('.')[0]}_decoded.json\"\n",
    "DECODED_OUTPUT = f\"{OUTPUT_FILE}\"\n",
    "with open(DECODED_OUTPUT, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data decoding Completed and saved at {DECODED_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6) Now to we have to  seprate Question and Answer,and save the data in CSV  file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the actual path to your JSON file\n",
    "JSON_INPUT_PATH = DECODED_OUTPUT\n",
    "# Path to save the output CSV file\n",
    "CSV_FILE_PATH = r\"Ram ki kahani.csv\"\n",
    "\n",
    "try:\n",
    "    # Open and load the JSON file\n",
    "    with open(JSON_INPUT_PATH, encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "\n",
    "        # Iterate through each entry in the JSON data\n",
    "        for entry in data:\n",
    "            # Iterate through each question block in the entry\n",
    "            for question_block in entry[\"questions\"]:\n",
    "                # Split the block into individual questions and answers\n",
    "                temp = question_block.split(\"\\n\\n\")\n",
    "                for qna in temp:\n",
    "                    qna_temp = qna.split(\"\\n\")\n",
    "                    # Ensure both question and answer are present\n",
    "                    if len(qna_temp) > 1:\n",
    "                        questions.append(qna_temp[0])\n",
    "                        answers.append(qna_temp[1])\n",
    "\n",
    "        print(\"Number of questions:\", len(questions))\n",
    "        print(\"Number of answers:\", len(answers))\n",
    "        \n",
    "        # Create and save the DataFrame if there are questions and answers\n",
    "        if questions and answers:\n",
    "            print(\"Creating DataFrame...\")\n",
    "            df = pd.DataFrame({'Questions': questions, 'Answers': answers})\n",
    "            print(\"Saving DataFrame to CSV file...\")\n",
    "            df.to_csv(CSV_FILE_PATH, index=False)\n",
    "            print(\"Data has been saved to\", CSV_FILE_PATH)\n",
    "\n",
    "        else:\n",
    "            print(\"No data to save.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: JSON file not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Error loading JSON data:\", str(e))\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7) Now to  clean  your  CSV  File we used re Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#   Function to remove Q1, A1, **Q1**, **A1** patterns and also asterisks from text\n",
    "def clean_text(text):\n",
    "    # Define the patterns\n",
    "    patterns = [r'Q\\d+:', r'A\\d+:', r'\\*\\*Q\\d+\\*\\*:', r'\\*\\*A\\d+\\*\\*:', r'\\*']\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to apply the clean_text function to each element in a row\n",
    "def clean_row(row):\n",
    "    return row.apply(clean_text)\n",
    "\n",
    "NEW_CSV_FILE_PATH = \"Cleaned_Ram_ki kahani.csv\"\n",
    "\n",
    "try:\n",
    "    # Load the CSV file\n",
    "    csv_data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "    # Apply the cleaning function to each cell in the DataFrame\n",
    "    cleaned_csv_data = csv_data.apply(clean_row, axis=1)\n",
    "\n",
    "    # Save the cleaned data back to a CSV file\n",
    "    cleaned_csv_data.to_csv(NEW_CSV_FILE_PATH, index=False)\n",
    "    print(f\"Cleaned data saved to {NEW_CSV_FILE_PATH}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {CSV_FILE_PATH} was not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The file is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
